{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8471f6",
   "metadata": {},
   "source": [
    "DATA FRAME: wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608ec3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70bc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0739445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Header Text\n",
      "0                                  Contents\n",
      "1                              Web scraping\n",
      "2                             History[edit]\n",
      "3                          Techniques[edit]\n",
      "4                Human copy-and-paste[edit]\n",
      "5               Text pattern matching[edit]\n",
      "6                    HTTP programming[edit]\n",
      "7                        HTML parsing[edit]\n",
      "8                         DOM parsing[edit]\n",
      "9                Vertical aggregation[edit]\n",
      "10    Semantic annotation recognizing[edit]\n",
      "11  Computer vision web-page analysis[edit]\n",
      "12                           Software[edit]\n",
      "13                       Legal issues[edit]\n",
      "14                      United States[edit]\n",
      "15                     European Union[edit]\n",
      "16                          Australia[edit]\n",
      "17                              India[edit]\n",
      "18    Methods to prevent web scraping[edit]\n",
      "19                           See also[edit]\n",
      "20                         References[edit]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the content of the wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all header tags and store their text in a list\n",
    "headers = []\n",
    "for header_tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "    headers.append(header_tag.text)\n",
    "\n",
    "# Create a data frame from the list of headers\n",
    "df = pd.DataFrame(headers, columns=[\"Header Text\"])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd674c8e",
   "metadata": {},
   "source": [
    "IMBD's top rated 50 movies: DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1a921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Name  Year Rating\n",
      "0    The Shawshank Redemption  1994    9.2\n",
      "1               The Godfather  1972    9.2\n",
      "2             The Dark Knight  2008    9.0\n",
      "3       The Godfather Part II  1974    9.0\n",
      "4                12 Angry Men  1957    9.0\n",
      "..                        ...   ...    ...\n",
      "245                  The Help  2011    8.0\n",
      "246                   Aladdin  1992    8.0\n",
      "247               Dersu Uzala  1975    8.0\n",
      "248                    Gandhi  1982    8.0\n",
      "249        Dances with Wolves  1990    8.0\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the content of the IMDb page\n",
    "url = \"https://www.imdb.com/chart/top\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all movie entries and store their name, rating, and year of release in a list\n",
    "movies = []\n",
    "for movie_entry in soup.find_all(\"td\", class_=\"titleColumn\"):\n",
    "    movie_name = movie_entry.find(\"a\").text\n",
    "    movie_year = movie_entry.find(\"span\").text[1:-1]\n",
    "    movie_rating = movie_entry.find_next(\"td\", class_=\"ratingColumn\").find(\"strong\").text\n",
    "    movies.append({\"Name\": movie_name, \"Year\": movie_year, \"Rating\": movie_rating})\n",
    "\n",
    "# Create a data frame from the list of movies\n",
    "df = pd.DataFrame(movies)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ec80a",
   "metadata": {},
   "source": [
    "IMBD's top rated 50 indian movies: DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    name rating  year\n",
      "0    Ramayana: The Legend of Prince Rama    8.5  1993\n",
      "1             Rocketry: The Nambi Effect    8.4  2022\n",
      "2                               Gol Maal    8.4  1979\n",
      "3                                Nayakan    8.4  1987\n",
      "4                             Anbe Sivam    8.4  2003\n",
      "..                                   ...    ...   ...\n",
      "245                       Poove Unakkaga    7.7  1996\n",
      "246                        Minnal Murali    7.7  2021\n",
      "247                    Thiruchitrambalam    7.7  2022\n",
      "248                           Goodachari    7.7  2018\n",
      "249                                 Joji    7.6  2021\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/india/top-rated-indian-movies/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "movies = []\n",
    "rows = soup.find(\"tbody\", class_=\"lister-list\").find_all(\"tr\")\n",
    "for row in rows:\n",
    "    movie = {}\n",
    "    movie[\"name\"] = row.find(\"td\", class_=\"titleColumn\").a.text\n",
    "    movie[\"rating\"] = row.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "    movie[\"year\"] = row.find(\"span\", class_=\"secondaryInfo\").text[1:5]\n",
    "    movies.append(movie)\n",
    "\n",
    "df = pd.DataFrame(movies)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd8355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Rajendra Prasad\n",
      "Term of office: 26 January 1950 to 13 May 1962\n",
      "\n",
      "Name: Dr. Sarvepalli Radhakrishnan\n",
      "Term of office: 13 May 1962 to 13 May 1967\n",
      "\n",
      "Name: Dr. Zakir Hussain\n",
      "Term of office: 13 May 1967 to 3 May 1969\n",
      "\n",
      "Name: Varahagiri Venkata Giri\n",
      "Term of office: 24 August 1969 to 24 August 1974\n",
      "\n",
      "Name: Fakhruddin Ali Ahmed\n",
      "Term of office: 24 August 1974 to 11 February 1977\n",
      "\n",
      "Name: Neelam Sanjiva Reddy\n",
      "Term of office: 25 July 1977 to 25 July 1982\n",
      "\n",
      "Name: Giani Zail Singh\n",
      "Term of office: 25 July 1982 to 25 July 1987\n",
      "\n",
      "Name: Ramaswamy Venkataraman\n",
      "Term of office: 25 July 1987 to 25 July 1992\n",
      "\n",
      "Name: Shankar Dayal Sharma\n",
      "Term of office: 25 July 1992 to 25 July 1997\n",
      "\n",
      "Name: Kocheril Raman Narayanan\n",
      "Term of office: 25 July 1997 to 25 July 2002\n",
      "\n",
      "Name: A.P.J. Abdul Kalam\n",
      "Term of office: 25 July 2002 to 25 July 2007\n",
      "\n",
      "Name: Pratibha Patil\n",
      "Term of office: 25 July 2007 to 25 July 2012\n",
      "\n",
      "Name: Pranab Mukherjee\n",
      "Term of office: 25 July 2012 to 25 July 2017\n",
      "\n",
      "Name: Ram Nath Kovind\n",
      "Term of office: 25 July 2017 to 25 July 2022\n",
      "\n",
      "Name: Droupadi Murmu\n",
      "Term of office: 25 July 2022 to present\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presidents = [    {\"Name\": \"Dr. Rajendra Prasad\", \"Term of office\": \"26 January 1950 to 13 May 1962\"},    {\"Name\": \"Dr. Sarvepalli Radhakrishnan\", \"Term of office\": \"13 May 1962 to 13 May 1967\"},    {\"Name\": \"Dr. Zakir Hussain\", \"Term of office\": \"13 May 1967 to 3 May 1969\"},    {\"Name\": \"Varahagiri Venkata Giri\", \"Term of office\": \"24 August 1969 to 24 August 1974\"},    {\"Name\": \"Fakhruddin Ali Ahmed\", \"Term of office\": \"24 August 1974 to 11 February 1977\"},    {\"Name\": \"Neelam Sanjiva Reddy\", \"Term of office\": \"25 July 1977 to 25 July 1982\"},    {\"Name\": \"Giani Zail Singh\", \"Term of office\": \"25 July 1982 to 25 July 1987\"},    {\"Name\": \"Ramaswamy Venkataraman\", \"Term of office\": \"25 July 1987 to 25 July 1992\"},    {\"Name\": \"Shankar Dayal Sharma\", \"Term of office\": \"25 July 1992 to 25 July 1997\"},    {\"Name\": \"Kocheril Raman Narayanan\", \"Term of office\": \"25 July 1997 to 25 July 2002\"},    {\"Name\": \"A.P.J. Abdul Kalam\", \"Term of office\": \"25 July 2002 to 25 July 2007\"},    {\"Name\": \"Pratibha Patil\", \"Term of office\": \"25 July 2007 to 25 July 2012\"},    {\"Name\": \"Pranab Mukherjee\", \"Term of office\": \"25 July 2012 to 25 July 2017\"},    {\"Name\": \"Ram Nath Kovind\", \"Term of office\": \"25 July 2017 to 25 July 2022\"}, {\"Name\": \"Droupadi Murmu\", \"Term of office\": \"25 July 2022 to present\"}]\n",
    "\n",
    "for president in presidents:\n",
    "    print(\"Name:\", president[\"Name\"])\n",
    "    print(\"Term of office:\", president[\"Term of office\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a1dd362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name                      Term of office\n",
      "0            Dr. Rajendra Prasad      26 January 1950 to 13 May 1962\n",
      "1   Dr. Sarvepalli Radhakrishnan          13 May 1962 to 13 May 1967\n",
      "2              Dr. Zakir Hussain           13 May 1967 to 3 May 1969\n",
      "3        Varahagiri Venkata Giri    24 August 1969 to 24 August 1974\n",
      "4           Fakhruddin Ali Ahmed  24 August 1974 to 11 February 1977\n",
      "5           Neelam Sanjiva Reddy        25 July 1977 to 25 July 1982\n",
      "6               Giani Zail Singh        25 July 1982 to 25 July 1987\n",
      "7         Ramaswamy Venkataraman        25 July 1987 to 25 July 1992\n",
      "8           Shankar Dayal Sharma        25 July 1992 to 25 July 1997\n",
      "9       Kocheril Raman Narayanan        25 July 1997 to 25 July 2002\n",
      "10            A.P.J. Abdul Kalam        25 July 2002 to 25 July 2007\n",
      "11                Pratibha Patil        25 July 2007 to 25 July 2012\n",
      "12              Pranab Mukherjee        25 July 2012 to 25 July 2017\n",
      "13               Ram Nath Kovind        25 July 2017 to 25 July 2022\n",
      "14                Droupadi Murmu             25 July 2022 to present\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "presidents = [    {\"Name\": \"Dr. Rajendra Prasad\", \"Term of office\": \"26 January 1950 to 13 May 1962\"},    {\"Name\": \"Dr. Sarvepalli Radhakrishnan\", \"Term of office\": \"13 May 1962 to 13 May 1967\"},    {\"Name\": \"Dr. Zakir Hussain\", \"Term of office\": \"13 May 1967 to 3 May 1969\"},    {\"Name\": \"Varahagiri Venkata Giri\", \"Term of office\": \"24 August 1969 to 24 August 1974\"},    {\"Name\": \"Fakhruddin Ali Ahmed\", \"Term of office\": \"24 August 1974 to 11 February 1977\"},    {\"Name\": \"Neelam Sanjiva Reddy\", \"Term of office\": \"25 July 1977 to 25 July 1982\"},    {\"Name\": \"Giani Zail Singh\", \"Term of office\": \"25 July 1982 to 25 July 1987\"},    {\"Name\": \"Ramaswamy Venkataraman\", \"Term of office\": \"25 July 1987 to 25 July 1992\"},    {\"Name\": \"Shankar Dayal Sharma\", \"Term of office\": \"25 July 1992 to 25 July 1997\"},    {\"Name\": \"Kocheril Raman Narayanan\", \"Term of office\": \"25 July 1997 to 25 July 2002\"},    {\"Name\": \"A.P.J. Abdul Kalam\", \"Term of office\": \"25 July 2002 to 25 July 2007\"},    {\"Name\": \"Pratibha Patil\", \"Term of office\": \"25 July 2007 to 25 July 2012\"},    {\"Name\": \"Pranab Mukherjee\", \"Term of office\": \"25 July 2012 to 25 July 2017\"},    {\"Name\": \"Ram Nath Kovind\", \"Term of office\": \"25 July 2017 to 25 July 2022\"}, {\"Name\": \"Droupadi Murmu\", \"Term of office\": \"25 July 2022 to present\"}]\n",
    "\n",
    "df = pd.DataFrame(presidents)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc9c08",
   "metadata": {},
   "source": [
    "ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a9fdb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC Men's ODI Team Rankings\n",
      "Rank\tTeam\tMatches\tPoints\tRating\n",
      "1\tIndia\n",
      "IND\t44\t5,010\t114\n",
      "2\tAustralia\n",
      "AUS\t32\t3,572\t112\n",
      "3\tNew Zealand\n",
      "NZ\t29\t3,229\t111\n",
      "4\tEngland\n",
      "ENG\t33\t3,656\t111\n",
      "5\tPakistan\n",
      "PAK\t25\t2,649\t106\n",
      "6\tSouth Africa\n",
      "SA\t27\t2,775\t103\n",
      "7\tBangladesh\n",
      "BAN\t33\t3,129\t95\n",
      "8\tSri Lanka\n",
      "SL\t34\t2,976\t88\n",
      "9\tAfghanistan\n",
      "AFG\t20\t1,419\t71\n",
      "10\tWest Indies\n",
      "WI\t41\t2,902\t71\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "teams = table.tbody.find_all(\"tr\")\n",
    "\n",
    "print(\"ICC Men's ODI Team Rankings\")\n",
    "print(\"Rank\\tTeam\\tMatches\\tPoints\\tRating\")\n",
    "\n",
    "for i, team in enumerate(teams[:10]):\n",
    "    tds = team.find_all(\"td\")\n",
    "    rank = tds[0].text.strip()\n",
    "    name = tds[1].text.strip()\n",
    "    matches = tds[2].text.strip()\n",
    "    points = tds[3].text.strip()\n",
    "    rating = tds[4].text.strip()\n",
    "    print(f\"{rank}\\t{name}\\t{matches}\\t{points}\\t{rating}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65030d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank              Team Matches Points Rating\n",
      "0    1        India\\nIND      44  5,010    114\n",
      "1    2    Australia\\nAUS      32  3,572    112\n",
      "2    3   New Zealand\\nNZ      29  3,229    111\n",
      "3    4      England\\nENG      33  3,656    111\n",
      "4    5     Pakistan\\nPAK      25  2,649    106\n",
      "5    6  South Africa\\nSA      27  2,775    103\n",
      "6    7   Bangladesh\\nBAN      33  3,129     95\n",
      "7    8     Sri Lanka\\nSL      34  2,976     88\n",
      "8    9  Afghanistan\\nAFG      20  1,419     71\n",
      "9   10   West Indies\\nWI      41  2,902     71\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i, team in enumerate(teams[:10]):\n",
    "    tds = team.find_all(\"td\")\n",
    "    rank = tds[0].text.strip()\n",
    "    name = tds[1].text.strip()\n",
    "    matches = tds[2].text.strip()\n",
    "    points = tds[3].text.strip()\n",
    "    rating = tds[4].text.strip()\n",
    "    data.append([rank, name, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Rank\", \"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402ce9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Player Team Rating\n",
      "0             Babar Azam  PAK    887\n",
      "1  Rassie van der Dussen   SA    787\n",
      "2           David Warner  AUS    747\n",
      "3        Quinton de Kock   SA    743\n",
      "4            Imam-ul-Haq  PAK    740\n",
      "5           Shubman Gill  IND    734\n",
      "6            Virat Kohli  IND    727\n",
      "7            Steve Smith  AUS    719\n",
      "8           Rohit Sharma  IND    719\n",
      "9        Kane Williamson   NZ    700\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "\n",
    "# Send a request to the website and retrieve the source code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the player rankings\n",
    "table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "# Extract the data from the table\n",
    "data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    data.append([player, team, rating])\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4523546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player Team Rating\n",
      "0     Mohammed Siraj  IND    729\n",
      "1     Josh Hazlewood  AUS    727\n",
      "2        Trent Boult   NZ    708\n",
      "3     Mitchell Starc  AUS    665\n",
      "4        Rashid Khan  AFG    659\n",
      "5         Adam Zampa  AUS    655\n",
      "6    Shakib Al Hasan  BAN    652\n",
      "7     Shaheen Afridi  PAK    641\n",
      "8  Mustafizur Rahman  BAN    638\n",
      "9   Mujeeb Ur Rahman  AFG    637\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "# Send a request to the website and retrieve the source code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the player rankings\n",
    "table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "# Extract the data from the table\n",
    "data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    data.append([player, team, rating])\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfaf074",
   "metadata": {},
   "source": [
    "ICC WOMEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f6d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC Women's ODI Team Rankings\n",
      "Rank\tTeam\tMatches\tPoints\tRating\n",
      "1\tAustralia\n",
      "AUS\t21\t3,603\t172\n",
      "2\tEngland\n",
      "ENG\t28\t3,342\t119\n",
      "3\tSouth Africa\n",
      "SA\t26\t3,098\t119\n",
      "4\tIndia\n",
      "IND\t27\t2,820\t104\n",
      "5\tNew Zealand\n",
      "NZ\t25\t2,553\t102\n",
      "6\tWest Indies\n",
      "WI\t27\t2,535\t94\n",
      "7\tBangladesh\n",
      "BAN\t13\t983\t76\n",
      "8\tThailand\n",
      "THA\t8\t572\t72\n",
      "9\tPakistan\n",
      "PAK\t27\t1,678\t62\n",
      "10\tSri Lanka\n",
      "SL\t8\t353\t44\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "teams = table.tbody.find_all(\"tr\")\n",
    "\n",
    "print(\"ICC Women's ODI Team Rankings\")\n",
    "print(\"Rank\\tTeam\\tMatches\\tPoints\\tRating\")\n",
    "\n",
    "for i, team in enumerate(teams[:10]):\n",
    "    tds = team.find_all(\"td\")\n",
    "    rank = tds[0].text.strip()\n",
    "    name = tds[1].text.strip()\n",
    "    matches = tds[2].text.strip()\n",
    "    points = tds[3].text.strip()\n",
    "    rating = tds[4].text.strip()\n",
    "    print(f\"{rank}\\t{name}\\t{matches}\\t{points}\\t{rating}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e1e62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank              Team Matches Points Rating\n",
      "0    1    Australia\\nAUS      21  3,603    172\n",
      "1    2      England\\nENG      28  3,342    119\n",
      "2    3  South Africa\\nSA      26  3,098    119\n",
      "3    4        India\\nIND      27  2,820    104\n",
      "4    5   New Zealand\\nNZ      25  2,553    102\n",
      "5    6   West Indies\\nWI      27  2,535     94\n",
      "6    7   Bangladesh\\nBAN      13    983     76\n",
      "7    8     Thailand\\nTHA       8    572     72\n",
      "8    9     Pakistan\\nPAK      27  1,678     62\n",
      "9   10     Sri Lanka\\nSL       8    353     44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i, team in enumerate(teams[:10]):\n",
    "    tds = team.find_all(\"td\")\n",
    "    rank = tds[0].text.strip()\n",
    "    name = tds[1].text.strip()\n",
    "    matches = tds[2].text.strip()\n",
    "    points = tds[3].text.strip()\n",
    "    rating = tds[4].text.strip()\n",
    "    data.append([rank, name, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Rank\", \"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9166846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Player Team Rating\n",
      "0         Alyssa Healy  AUS    762\n",
      "1          Beth Mooney  AUS    754\n",
      "2      Laura Wolvaardt   SA    732\n",
      "3       Natalie Sciver  ENG    731\n",
      "4          Meg Lanning  AUS    717\n",
      "5     Harmanpreet Kaur  IND    716\n",
      "6      Smriti Mandhana  IND    714\n",
      "7       Rachael Haynes  AUS    680\n",
      "8  Chamari Athapaththu   SL    655\n",
      "9    Amy Satterthwaite   NZ    641\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "\n",
    "# Send a request to the website and retrieve the source code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the player rankings\n",
    "table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "# Extract the data from the table\n",
    "data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    data.append([player, team, rating])\n",
    "\n",
    "# Create a DataFrame \n",
    "df = pd.DataFrame(data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a70d5843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player Team Rating\n",
      "0   Hayley Matthews   WI    373\n",
      "1    Natalie Sciver  ENG    371\n",
      "2      Ellyse Perry  AUS    366\n",
      "3    Marizanne Kapp   SA    349\n",
      "4       Amelia Kerr   NZ    336\n",
      "5     Deepti Sharma  IND    322\n",
      "6  Ashleigh Gardner  AUS    292\n",
      "7     Jess Jonassen  AUS    250\n",
      "8          Nida Dar  PAK    232\n",
      "9    Jhulan Goswami  IND    214\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "# Send a request to the website and retrieve the source code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the player rankings\n",
    "table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "# Extract the data from the table\n",
    "data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    data.append([player, team, rating])\n",
    "\n",
    "# Create a DataFrame \n",
    "df = pd.DataFrame(data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a4c14",
   "metadata": {},
   "source": [
    "NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fc65310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Time</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google employees slam CEO Sundar Pichai for ‘r...</td>\n",
       "      <td>19 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift says fans will 'get on it' to red...</td>\n",
       "      <td>24 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wall Street’s favorite sports-betting stocks a...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here’s how much money Americans saved at every...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Gates: Nuclear waste is not a reason to a...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks making the biggest moves midday: Lyft, ...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 investing lessons from the Club’s meeting Fr...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Astronaut Sen. Kelly touts 'stunning' space bu...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Air Force's aging E-3 Sentry presents airborne...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FBI is searching home of Mike Pence for more c...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We're buying the dip in a beauty stock that st...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>National Archives, museum sued for telling vis...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Here's what's happening with home prices right...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Amazon is the latest threat to Facebook as ad ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This is how Microsoft and Alphabet stack up ag...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>These are the best deals on TVs ahead of Super...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Top House Republicans scrutinize SEC investiga...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Steve Wozniak warns: ChatGPT is 'useful,' but ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why you won't see many car ads during Sunday's...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How this 39-year-old earns $26,000 a year in C...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Goldman sees big layoff announcements as 'a ri...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>An 85-year Harvard study found the No. 1 thing...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A.I. search could reap Microsoft $10 billion a...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Judge extends deadline in lawsuit seeking to p...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The hidden price of eggs: How high costs may t...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Where EV trucks are going to hit the road first</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SEC considers 'adjustments' to controversial c...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bill Gates thinks A.I. like ChatGPT is the 'mo...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hybrid workers earn more than remote and in-pe...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Analyst Jonas says investors may have missed t...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/02/10/morgan-stanley...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headlines         Time  \\\n",
       "0   Google employees slam CEO Sundar Pichai for ‘r...   19 Min Ago   \n",
       "1   Taylor Swift says fans will 'get on it' to red...   24 Min Ago   \n",
       "2   Wall Street’s favorite sports-betting stocks a...   1 Hour Ago   \n",
       "3   Here’s how much money Americans saved at every...   1 Hour Ago   \n",
       "4   Bill Gates: Nuclear waste is not a reason to a...   1 Hour Ago   \n",
       "5   Stocks making the biggest moves midday: Lyft, ...   1 Hour Ago   \n",
       "6   3 investing lessons from the Club’s meeting Fr...   1 Hour Ago   \n",
       "7   Astronaut Sen. Kelly touts 'stunning' space bu...  2 Hours Ago   \n",
       "8   Air Force's aging E-3 Sentry presents airborne...  2 Hours Ago   \n",
       "9   FBI is searching home of Mike Pence for more c...  2 Hours Ago   \n",
       "10  We're buying the dip in a beauty stock that st...  2 Hours Ago   \n",
       "11  National Archives, museum sued for telling vis...  2 Hours Ago   \n",
       "12  Here's what's happening with home prices right...  2 Hours Ago   \n",
       "13  Amazon is the latest threat to Facebook as ad ...  2 Hours Ago   \n",
       "14  This is how Microsoft and Alphabet stack up ag...  2 Hours Ago   \n",
       "15  These are the best deals on TVs ahead of Super...  2 Hours Ago   \n",
       "16  Top House Republicans scrutinize SEC investiga...  2 Hours Ago   \n",
       "17  Steve Wozniak warns: ChatGPT is 'useful,' but ...  2 Hours Ago   \n",
       "18  Why you won't see many car ads during Sunday's...  3 Hours Ago   \n",
       "19  How this 39-year-old earns $26,000 a year in C...  3 Hours Ago   \n",
       "20  Goldman sees big layoff announcements as 'a ri...  3 Hours Ago   \n",
       "21  An 85-year Harvard study found the No. 1 thing...  3 Hours Ago   \n",
       "22  A.I. search could reap Microsoft $10 billion a...  4 Hours Ago   \n",
       "23  Judge extends deadline in lawsuit seeking to p...  4 Hours Ago   \n",
       "24  The hidden price of eggs: How high costs may t...  4 Hours Ago   \n",
       "25    Where EV trucks are going to hit the road first  4 Hours Ago   \n",
       "26  SEC considers 'adjustments' to controversial c...  4 Hours Ago   \n",
       "27  Bill Gates thinks A.I. like ChatGPT is the 'mo...  4 Hours Ago   \n",
       "28  Hybrid workers earn more than remote and in-pe...  4 Hours Ago   \n",
       "29  Analyst Jonas says investors may have missed t...  4 Hours Ago   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "1   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "2   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "3   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "4   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "5   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "6   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "7   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "8   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "9   https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "10  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "11  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "12  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "13  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "14  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "15  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "16  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "17  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "18  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "19  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "20  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "21  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "22  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "23  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "24  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "25  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "26  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "27  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "28  https://www.cnbc.com/2023/02/10/morgan-stanley...  \n",
       "29  https://www.cnbc.com/2023/02/10/morgan-stanley...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send url request and download page source code\n",
    "News_soup=BeautifulSoup(requests.get(\"https://www.cnbc.com/world/?region=world\").content,\"html.parser\")\n",
    "\n",
    "# Scrap Headlines\n",
    "Headline=[i.text for i in News_soup.find_all(class_=\"LatestNews-headline\") ] \n",
    "\n",
    "# Scrap Time\n",
    "Time=[i.text for i in News_soup.find_all(\"span\",class_=\"LatestNews-wrapper\") ] \n",
    "\n",
    "# Scrap news Links\n",
    "for links in News_soup.find_all(\"a\",class_=\"LatestNews-headline\"):\n",
    "    links=links.get('href')\n",
    "\n",
    "# Create data frame\n",
    "news1=pd.DataFrame({'Headlines':Headline,'Time':Time,\"Link\":links})\n",
    "news1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4f8fc",
   "metadata": {},
   "source": [
    "ARTICLE FROM AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e585473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors            Date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "3                                 Boden, Margaret A.      August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                        Miller, Tim    February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                      Kohavi, Ron, John, George H.    December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    html_content = response.content\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find all the articles on the page\n",
    "    articles = soup.find_all(\"article\")\n",
    "    titles = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "    links = []\n",
    "\n",
    "    # Loop through each article\n",
    "    for article in articles:\n",
    "        #print(article)\n",
    "        # Extract the paper title\n",
    "        title = article.find(\"h2\", class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\").text\n",
    "        titles.append(title)\n",
    "\n",
    "        # Extract the authors\n",
    "        author = article.find(\"span\", class_=\"sc-1w3fpd7-0 dnCnAO\").text\n",
    "        authors.append(author)\n",
    "\n",
    "        # # Extract the published date\n",
    "        date = article.find(\"span\", class_=\"sc-1thf9ly-2 dvggWt\").text\n",
    "        dates.append(date)\n",
    "\n",
    "        # # Extract the paper URL\n",
    "        paper_url = article.find(\"a\", class_=\"sc-5smygv-0 fIXTHm\")[\"href\"]\n",
    "        links.append(paper_url)\n",
    "# Create DataFrame\n",
    "    df = pd.DataFrame({\"Title\": titles, \"Authors\": authors, \"Date\": dates, \"Link\": links})\n",
    "    print(df)\n",
    "else:\n",
    "    # If the request was not successful, print an error message\n",
    "    print(\"Failed to fetch the URL. Please check the URL and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2a8d1",
   "metadata": {},
   "source": [
    "RESTAURANTS DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b97311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle BarbequeConnaught Place</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree3CS Mall</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe KnoshThe Leela Ambience Convention Hotel</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle BarbequePacific Mall</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque CompanyGardens Galleria</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India GrillHilton Garden Inn</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi BarbequeTaurus Sarovar Portico</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que VillageIndirapuram Ha...</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill RoomSuncity Business Tower</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Restaurant Name  \\\n",
       "0                     Castle BarbequeConnaught Place   \n",
       "1                            Jungle Jamboree3CS Mall   \n",
       "2      Cafe KnoshThe Leela Ambience Convention Hotel   \n",
       "3                        Castle BarbequePacific Mall   \n",
       "4               The Barbeque CompanyGardens Galleria   \n",
       "5                       India GrillHilton Garden Inn   \n",
       "6               Delhi BarbequeTaurus Sarovar Portico   \n",
       "7  The Monarch - Bar Be Que VillageIndirapuram Ha...   \n",
       "8            Indian Grill RoomSuncity Business Tower   \n",
       "\n",
       "                         Cuisine  \\\n",
       "0          Chinese, North Indian   \n",
       "1   North Indian, Asian, Italian   \n",
       "2           Italian, Continental   \n",
       "3          Chinese, North Indian   \n",
       "4          North Indian, Chinese   \n",
       "5          North Indian, Italian   \n",
       "6                   North Indian   \n",
       "7                   North Indian   \n",
       "8          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Rating  \\\n",
       "0                     Connaught Place, Central Delhi    4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida      4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.6   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "\n",
       "                                           Image_url  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to the webpage server to get the source code of the page and download the page content\n",
    "soup=BeautifulSoup(requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special').content,'html.parser')\n",
    "\n",
    "# Scrape Restaurant name\n",
    "name=[i.text.split(',')[0] for i in soup.find_all('div',class_='restnt-info cursor')]\n",
    "\n",
    "# Scrape cuisine\n",
    "cuisine=[i.text.split('|')[1] for i in soup.find_all('span',class_=\"double-line-ellipsis\")] \n",
    "\n",
    "# Scrape location\n",
    "location=[i.text for i in soup.find_all('div',class_='restnt-loc ellipsis')] \n",
    "\n",
    "# Scrape rating\n",
    "rating=[i.text for i in soup.find_all('div',class_='restnt-rating rating-4')]\n",
    "\n",
    "# Scrap url of image\n",
    "image=[i['data-src'] for i in soup.find_all(\"img\", class_=\"no-img\")] \n",
    "\n",
    "# Create DataFrame\n",
    "df=pd.DataFrame({'Restaurant Name':name,'Cuisine':cuisine,'Location':location,\"Rating\":rating,\"Image_url\": image})\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
